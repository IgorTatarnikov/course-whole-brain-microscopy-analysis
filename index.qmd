---
title: Running pose estimation on the SWC HPC system
author: Adam Tyson & Niko Sirmpilatze
execute: 
  enabled: true
format:
    revealjs:
        theme: [default, niu-dark.scss]
        logo: img/logo_niu_dark.png
        footer: "SWC | 2023-12-06"
        slide-number: c
        menu:
            numbers: true
        chalkboard: true
        scrollable: true
        preview-links: false
        view-distance: 10
        mobile-view-distance: 10
        auto-animate: true
        auto-play-media: true
        code-overflow: wrap
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: arial
          curve: linear
    html:
        theme: [default, niu-dark.scss]
        logo: img/logo_niu_dark.png
        date: "2023-12-06"
        toc: true
        code-overflow: scroll
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: arial
          curve: linear
          margin-left: 0
        embed-resources: true
        page-layout: full
my-custom-stuff:
   my-reuseable-variable: "I can use this wherever I want in the markdown, and change it in only once place :)"
---


## Contents

* Introduction to High Performance Computing
* SWC HPC system
* Using SLURM
* Running pose estimation on the SWC HPC

## Introduction to High Performance Computing (HPC)
* Lots of meanings
* Often just a system with many machines (nodes) linked together with some/all of:
  * Lots of CPU cores per node
  * Powerful GPUs
  * Lots of memory per node
  * Fast networking to link nodes
  * Fast data storage
  * Standardised software installation

## Why
* Run jobs too large for desktop workstations
* Run many jobs at once
* Efficiency (cheaper to have central machines running 24/7)

* In neuroscience, typically used for:
  * Analysing large data (e.g. high memory requirements)
  * Paralellising analysis/modelling (run on many machines at once)


## SWC HPC hardware
(Correct at time of writing)

* Ubuntu 20.04
* 81 nodes
  * 46 CPU nodes
  * 35 GPU nodes
* 3000 CPU cores
* 83 GPUs
* ~20TB RAM

## SWC HPC nodes

## SWC HPC software

## SLURM

