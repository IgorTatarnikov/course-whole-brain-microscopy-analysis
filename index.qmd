---
title: Running pose estimation on the SWC HPC system
author: Adam Tyson, Niko Sirmpilatze & Laura Porta
execute: 
  enabled: true
format:
    revealjs:
        theme: [default, niu-dark.scss]
        logo: img/logo_niu_dark.png
        footer: "SWC | 2023-12-06"
        slide-number: c
        menu:
            numbers: true
        chalkboard: true
        scrollable: true
        preview-links: false
        view-distance: 10
        mobile-view-distance: 10
        auto-animate: true
        auto-play-media: true
        code-overflow: wrap
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: arial
          curve: linear
    html:
        theme: [default, niu-dark.scss]
        logo: img/logo_niu_dark.png
        date: "2023-12-06"
        toc: true
        code-overflow: scroll
        highlight-style: atom-one
        mermaid: 
          theme: neutral
          fontFamily: arial
          curve: linear
          margin-left: 0
        embed-resources: true
        page-layout: full
my-custom-stuff:
   my-reuseable-variable: "I can use this wherever I want in the markdown, and change it in only once place :)"
---


## Contents

* Introduction to High Performance Computing
* SWC HPC system
* Using the job scheduler
* Running pose estimation on the SWC HPC

## Introduction to High Performance Computing (HPC)
* Lots of meanings
* Often just a system with many machines (nodes) linked together with some/all of:
  * Lots of CPU cores per node
  * Powerful GPUs
  * Lots of memory per node
  * Fast networking to link nodes
  * Fast data storage
  * Standardised software installation

## Why
* Run jobs too large for desktop workstations
* Run many jobs at once
* Efficiency (cheaper to have central machines running 24/7)

* In neuroscience, typically used for:
  * Analysing large data (e.g. high memory requirements)
  * Paralellising analysis/modelling (run on many machines at once)


## SWC HPC hardware
(Correct at time of writing)

* Ubuntu 20.04
* 81 nodes
  * 46 CPU nodes
  * 35 GPU nodes
* 3000 CPU cores
* 83 GPUs
* ~20TB RAM

## SWC HPC nodes

## Logging in

Log into bastion node (not necessary within SWC)
```bash
ssh <USERNAME>@ssh.swc.ucl.ac.uk
```

Log into HPC gateway node
```bash
ssh <USERNAME>@hpc-gw1.hpc.swc.ucl.ac.uk
```

## File systems

* `/nfs/nhome` or `/nfs/ghome` - "home drive" (SWC/GCNU)
* `/nfs/winstor/<group>` - "Old" research data storage (read-only soon)
*`/nfs/gatsbystor` - GCNU data storage
*`/ceph<group>` - Current research data storage
*`/ceph/scratch` - Not backed up, for short-term storage
*`ceph/apps` - HPC applications

::: {.callout-note}
You may only be able to "see" a drive if you navigate to it
:::

## HPC software
All nodes have the same software installed
* Ubuntu 20.04 LTS
* General linux utilities

## Modules
Preinstalled packages available for use, including:

* BrainGlobe
* CUDA
* DeepLabCut
* Julia
* Kilosort
* mamba
* MATLAB
* miniconda
* SLEAP

## Using modules

List available modules
```bash
module avail
```

Load a module

```bash
module load SLEAP
```

Unload a module

```bash
module unload SLEAP
```

Load a specific version
```bash
module load SLEAP/2023-08-01
```

List loaded modules
```bash
module list
```


## SLURM

